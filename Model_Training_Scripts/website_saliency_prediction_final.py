# -*- coding: utf-8 -*-
"""WEBSITE SALIENCY PREDICTION_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OiG7piLUha7NADIgeax8bdIIeiZ_twt9

# SCRIPT NAME: WEBSITE SALIENCY PREDICTION_Final.ipynb
THIS SCRIPT DOES THE FOLLOWING FUNCTIONALITIES:

          1. PRE_PROCESS FIWI DATASET
          2. TRAIN THE FCN-16s MODEL ON FIWI DATA
          3. VISIALIZE TRAINING PEFORMANCE
          4. SAVE THE MODEL FOR LATER USE
"""

# import libraries
import tensorflow
import keras

# comment this part if not using google colab
# from google.colab import drive
# drive.mount('/content/drive')

"""## Data Visualisation"""

import random
import cv2, os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.image as mpimg

# Path to the data
dir_data_X_val = "/content/drive/My Drive/HCI_prep/Dataset_Website1/stimuli/"
dir_data_Y_val = "/content/drive/My Drive/HCI_prep/Dataset_Website1/finalHeatMap/"

ldseg = os.listdir(dir_data_Y_val)
ldseg.sort()
ldseg = np.array(ldseg)

ldseg_tr = os.listdir(dir_data_X_val)
ldseg_tr.sort()
ldseg_tr = np.array(ldseg_tr)

# pick the first image from FiWi Dataset and Visualise
fnm = ldseg[0]
fnm_tr = ldseg_tr[0]

print(fnm)
print(fnm_tr)

img = mpimg.imread(dir_data_Y_val + fnm)
imgplot = plt.imshow(img)
plt.title("Saliency image for Validation: FiWi Dataset ")
plt.show()

img = mpimg.imread(dir_data_X_val + fnm_tr)
imgplot = plt.imshow(img)
plt.title("Original image for Validation: FiWi Dataset")
plt.show()

"""# Data Preprocessing"""

from PIL import Image

n_classes = 1
meanval = (104.00699, 116.66877, 122.67892)
# meanval= (192.19138, 191.60353, 190.41075)

output_width, output_height = 224, 224
input_width, input_height = 224, 224

"""
Function Name: ImageArr()
Parameters: path, width, height
Functionalities: 1) Resizing to 224*224
                 2) Mean Value Reduction
                 3) RGB to BGR conversion

Returns: preprocessed image

"""


def ImageArr(path, width, height):
    # print(path)
    img = cv2.imread(path, 1)
    # resize image
    img = np.float32(cv2.resize(img, (width, height)))  # / 127.5 - 1
    # RGB to BGR and Meanvalue reduction
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    img -= meanval
    plt.imshow(img)
    plt.show()
    return img


"""
Function Name: ImpArr()
Parameters: path,class, width, height
Functionalities: 1) BGR to GRAY conversion
                 2) Resizing to 224*224
                 2) values range from 0 to 255

Returns: preprocessed ground truth

"""


def ImpArr(path, classes, width, height):
    # ACCORDIG TO THE REF PAPER
    img = cv2.imread(path, 1)

    # convert to gray image
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Resize to 224*224
    img = cv2.resize(img, (224, 224))

    # values range from 0 to 255
    label = np.array(img, dtype=np.uint8)

    # For classifiction problem, take the values of saliency map as positive sample,
    # if the value is more than 1/6 of 255
    label = label > 255.0 * 1 / 6
    # print(label)
    plt.imshow(label)
    # plt.title("after scaling")
    plt.show()
    # print(label)
    label = np.expand_dims(label, axis=-1)
    # print("aftr expanding dim",label.shape)

    return label


images_data = os.listdir("/content/drive/My Drive/HCI_prep/Dataset_Website1/stimuli/")
images_data.sort()
print("sorted", images_data)

saliency_data = os.listdir("/content/drive/My Drive/HCI_prep/Dataset_Website1/finalHeatMap/")
saliency_data.sort()
print("sorted", saliency_data)

# Append processed image arrays in separate lists

X = []
Y = []

for im, seg in zip(images_data, saliency_data):
    X.append(ImageArr(dir_data_X_val + im, input_width, input_height))
    Y.append(ImpArr(dir_data_Y_val + seg, n_classes, output_width, output_height))

X, Y = np.array(X), np.array(Y)
print(X.shape, Y.shape)

"""# FCN-16 model building
Although pretrained network is used, there are a few small modification than previous GDI model. So, the network needs to be defined again
"""

# import libraries for training the model
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session
import keras, sys, time, warnings
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
import pandas as pd

'''
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
config =  tf.compat.v1.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.95
config.gpu_options.visible_device_list = "2"
'''
set_session(tf.compat.v1.Session())

print("python {}".format(sys.version))
print("keras version {}".format(keras.__version__));
del keras
print("tensorflow version {}".format(tf.__version__))

"""
Function Name: crop()
Parameters: feature map1, feature map 2,image input
Functionalities: crop feature map1 wrt.feature map 2
Returns: cropped o1 and o2
"""


def crop(o1, o2, i):
    o_shape2 = Model(i, o2).output_shape

    if IMAGE_ORDERING == 'channels_first':
        output_height2 = o_shape2[2]
        output_width2 = o_shape2[3]
    else:
        output_height2 = o_shape2[1]
        output_width2 = o_shape2[2]

    o_shape1 = Model(i, o1).output_shape
    if IMAGE_ORDERING == 'channels_first':
        output_height1 = o_shape1[2]
        output_width1 = o_shape1[3]
    else:
        output_height1 = o_shape1[1]
        output_width1 = o_shape1[2]

    cx = abs(output_width1 - output_width2)
    cy = abs(output_height2 - output_height1)

    if output_width1 > output_width2:
        o1 = Cropping2D(cropping=((0, 0), (0, cx)),
                        data_format=IMAGE_ORDERING)(o1)
    else:
        o2 = Cropping2D(cropping=((0, 0), (0, cx)),
                        data_format=IMAGE_ORDERING)(o2)

    if output_height1 > output_height2:
        o1 = Cropping2D(cropping=((0, cy), (0, 0)),
                        data_format=IMAGE_ORDERING)(o1)
    else:
        o2 = Cropping2D(cropping=((0, cy), (0, 0)),
                        data_format=IMAGE_ORDERING)(o2)

    return o1, o2


import tensorflow.keras
from tensorflow.keras.models import *
from tensorflow.keras.layers import *

IMAGE_ORDERING = 'channels_last'

"""
Function Name: get_vgg_encoder()
Functionalities: This function defines the VGG encoder part of the FCN network
Parameter:input_height=224,  input_width=224
Returns: final layer of every blocks as f1,f2,f3,f4,f5

"""


def get_vgg_encoder(input_height=224, input_width=224):
    pad = 1

    # heights and weights must be divided by 32, for fcn
    assert input_height % 32 == 0
    assert input_width % 32 == 0

    img_input = Input(shape=(input_height, input_width, 3))

    # Unlike base paper, stride=1 has not been used here, because
    # Keras has default stride=1

    x = (ZeroPadding2D((pad, pad), data_format=IMAGE_ORDERING))(img_input)
    x = Conv2D(64, (3, 3), activation='relu', padding='valid', name='block1_conv1', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format=IMAGE_ORDERING)(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format=IMAGE_ORDERING)(x)
    f1 = x

    # Block 2
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format=IMAGE_ORDERING)(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format=IMAGE_ORDERING)(x)
    f2 = x

    # Block 3
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format=IMAGE_ORDERING)(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format=IMAGE_ORDERING)(x)
    f3 = x

    # Block 4
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format=IMAGE_ORDERING)(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format=IMAGE_ORDERING)(x)
    f4 = x

    # Block 5
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format=IMAGE_ORDERING)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format=IMAGE_ORDERING)(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format=IMAGE_ORDERING)(x)
    f5 = x

    return img_input, [f1, f2, f3, f4, f5]


"""
Function Name: fcn_16()
Functionalities: This function defines the Fully Convolutional part of the FCN network
                 and adds skip connections to build FCN-16 network
Parameter:n_classes, encoder=get_vgg_encoder, input_height=224,input_width=224
Returns: model

"""


def fcn_16(n_classes, encoder=get_vgg_encoder, input_height=224, input_width=224):
    # Take levels from the base model, i.e. vgg
    img_input, levels = encoder(input_height=input_height, input_width=input_width)
    [f1, f2, f3, f4, f5] = levels

    o = f5

    # fcn6
    o = (Conv2D(4096, (7, 7), activation='relu', padding='same', name="fc6", data_format=IMAGE_ORDERING))(o)
    o = Dropout(0.4, name="dropout_fc6")(o)

    # fc7
    o = (Conv2D(4096, (1, 1), activation='relu', padding='same', name="fc7", data_format=IMAGE_ORDERING))(o)

    o = Dropout(0.4, name="dropout_fc7")(o)

    conv7 = (Conv2D(1, (1, 1), activation='relu', padding='same', name="score_salw", data_format=IMAGE_ORDERING))(o)
    # conv7 = Dropout(0.15 ,name= "dropout_conv7" )(conv7)

    conv7_4 = Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', name="upscore_sal2w",
                              use_bias=False, data_format=IMAGE_ORDERING)(conv7)
    # conv7_4= Dropout(0.3)(conv7_4)

    pool411 = (Conv2D(1, (1, 1), activation='relu', padding='same', name="score_pool4w", data_format=IMAGE_ORDERING))(
        f4)

    # Add a crop layer 
    o, o2 = crop(pool411, conv7_4, img_input)

    o = Add(name="Add_Layer")([o, o2])

    # add skip connection
    # o = Add(name="add")([pool411, conv7_4])

    # 16 x upsample
    o = Conv2DTranspose(n_classes, kernel_size=(32, 32), strides=(16, 16), name="upsample", use_bias=False,
                        data_format=IMAGE_ORDERING)(o)

    # crop layer
    # Caffe calls crop layer that takes o and img_input as argument, it takes their difference and crops
    # But keras takes it as touple, I checked the size diff and put this value manually.
    # output dim was 240 , input dim was 224. 240-224=16. so 16/2=8

    score = Cropping2D(cropping=((8, 8), (8, 8)), data_format=IMAGE_ORDERING)(o)

    o = (Activation('sigmoid'))(score)
    model = Model(img_input, o)

    model.model_name = "fcn_16"

    return model


# Binary classification problem with 2 classes, salient or non-salient
model = fcn_16(n_classes=1, encoder=get_vgg_encoder, input_height=224,
               input_width=224)

# show model summary
model.summary()

"""# Loading the Pre-Trained model trained, on GDI Dataset and Freeze some layers"""

from tensorflow.keras.models import load_model

# path for pretrained GDI model
pretrained_GDI = "/content/drive/My Drive/HCI_prep/with_VGG_weights_gdi_fcn16_24_02_2020.h5"  # with_VGG_weights_gdi_fcn16_20_01_2020.h5"
model_pretrained = load_model(pretrained_GDI)

# Set the weights
model.set_weights(model_pretrained.get_weights())

# Check the layers and the weights
# Cros-validate the weights manually with the vgg weights opened with Netron API
# model.set_weights(model_pretrained)
for i, layer in enumerate(model.layers):
    print(i, layer.name)
    print(layer.get_weights())
    # print(i,layer.name)

# Freeze the first 19 layers
for layer in model.layers[:19]:
    # print(layer.name)
    layer.trainable = False

# Check trainable status, i.e. frozen layers
for i, layer in enumerate(model.layers):
    print("Layer:", i, " layer name:", layer.name, " Trainable Status: ", layer.trainable)

"""# Compile and Train the Model"""

from tensorflow.keras import optimizers

# Customise the optimiser

# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
sgd = optimizers.SGD(lr=0.0001, momentum=0.90, decay=5 ** (-4), nesterov=True)
adam = optimizers.Adam(learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)

# Compile the model 
# Try with adam with custom sgd (SGD+nesterov) and choose the best
# default adam: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.
# For Loss: chcek with mse, binary crossentropy , kullback_leibler_divergence

model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])

# Check the final model Summary: Check non trainable parameters
model.summary()

"""# Start Training"""

# Fit the model
hist1 = model.fit(X, Y, validation_split=0.2, shuffle=True, batch_size=12, epochs=60,
                  verbose=1)  # , callbacks=[es, mc])
# Save the model
model.save("/content/drive/My Drive/HCI_prep/with_weights_FiWi_fcn_16_latest.h5")

"""# Visualizing the model performance"""

# Plot Training Loss, Validation Loss, Training Acuracy and Validation Accuracy for all the epochs
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()
c = 0
col = ["r", "g", "b", "c"]
for key1 in ['loss', 'val_loss']:
    # print(hist1.history[key1])
    ax1.plot(hist1.history[key1], label=key1, c=col[c])
    ax1.legend(loc=2)
    c = c + 1
for key2 in ['acc', 'val_acc']:
    ax2.plot(hist1.history[key2], label=key2, c=col[c])
    ax2.legend(loc=1)
    c = c + 1

ax1.set_xlabel('epochs')
ax1.set_ylabel('Loss', color='r')
ax2.set_ylabel('Accuracy*100', color='b')
plt.title("Model Performance")
plt.savefig("/content/drive/My Drive//HCI_prep/loss_27_02_20_Final.png")
